---
AWSTemplateFormatVersion: "2010-09-09"
Description: Full bulk_load Lab template, bucket/table/role/function automatically created
Parameters:
  BucketName:
    Type: String
    Default: "thisisatestbucketnameneedstobeunique"
Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "Bucket Name"
        Parameters:
          - BucketName
    ParameterLabels:
      BucketName:
        default: "Pick a globally unique name for your source S3 bucket"
Resources:
  SourceBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    Properties:
      BucketName: !Ref BucketName
      NotificationConfiguration:
        LambdaConfigurations:
          -
            Function: !GetAtt BulkLoadLambda.Arn
            Event: "s3:ObjectCreated:*"
            Filter:
              S3Key:
                Rules:
                  -
                    Name: suffix
                    Value: csv
  BucketPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref BulkLoadLambda
      Principal: s3.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !Sub "arn:aws:s3:::${BucketName}"
  BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref SourceBucket
      PolicyDocument:
        Statement:
          -
            Action:
              - "s3:GetObject"
            Effect: "Allow"
            Resource: !Sub "arn:aws:s3:::${SourceBucket}/*"
            Principal:
              AWS: !GetAtt LambdaDBRole.Arn
  LambdaDBRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: base
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                Resource: '*'
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                  - "s3:HeadObject"
                Resource: !Sub
                  - 'aws:aws:s3:::${BucketName}/*'
                  - BucketName: !Ref BucketName
              - Effect: Allow
                Action:
                  - "dynamodb:*"
                Resource: !GetAtt DynamoMovieTable.Arn
  DynamoMovieTable:
    Type: AWS::DynamoDB::Table
    Properties:
      AttributeDefinitions:
        -
          AttributeName: 'Year'
          AttributeType: 'N'
        -
          AttributeName: 'Title'
          AttributeType: 'S'
      KeySchema:
        -
          AttributeName: 'Year'
          KeyType: HASH
        -
          AttributeName: 'Title'
          KeyType: RANGE
      TableName: 'Movies'
      ProvisionedThroughput:
        ReadCapacityUnits: 5
        WriteCapacityUnits: 5
  BulkLoadLambda:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.7
      Timeout: 60
      MemorySize: 192
      Role: !GetAtt LambdaDBRole.Arn
      Handler: index.lambda_handler
      Code:
        ZipFile: |
          import csv
          import os
          import tempfile

          import boto3

          dynamodb = boto3.resource('dynamodb')
          table = dynamodb.Table('Movies')
          s3 = boto3.client('s3')


          def lambda_handler(event, context):

              for record in event['Records']:
                  source_bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  with tempfile.TemporaryDirectory() as tmpdir:
                      download_path = os.path.join(tmpdir, key)
                      s3.download_file(source_bucket, key, download_path)
                      items = read_csv(download_path)

                      with table.batch_writer() as batch:
                          for item in items:
                              batch.put_item(Item=item)


          def read_csv(file):
              items = []
              with open(file) as csvfile:
                  reader = csv.DictReader(csvfile)
                  for row in reader:
                      data = {}
                      data['Meta'] = {}
                      data['Year'] = int(row['Year'])
                      data['Title'] = row['Title'] or None
                      data['Meta']['Length'] = int(row['Length'] or 0)
                      data['Meta']['Subject'] = row['Subject'] or None
                      data['Meta']['Actor'] = row['Actor'] or None
                      data['Meta']['Actress'] = row['Actress'] or None
                      data['Meta']['Director'] = row['Director'] or None
                      data['Meta']['Popularity'] = row['Popularity'] or None
                      data['Meta']['Awards'] = row['Awards'] == 'Yes'
                      data['Meta']['Image'] = row['Image'] or None
                      data['Meta'] = {k: v for k,
                                      v in data['Meta'].items() if v is not None}
                      items.append(data)
              return items
      Description: Bulk load lambda functions for csv uploads to our bucket
Outputs:
  BucketARN:
    Value: !GetAtt SourceBucket.Arn
  DynamoARN:
    Value: !GetAtt DynamoMovieTable.Arn
  RoleARN:
    Value: !GetAtt LambdaDBRole.Arn
